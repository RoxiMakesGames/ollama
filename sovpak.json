{
  "$schema": "https://sovereign.dev/schemas/package/1.0.0",
  "id": "ollama",
  "name": "Ollama LLM",
  "version": "1.0.0",
  "description": "Local LLM inference via Ollama â€” models, chat, and API access",
  "license": "AGPL-3.0",
  "author": {
    "name": "Sovereign Infrastructure Team",
    "url": "https://opensovereign.dev"
  },
  "kind": "package",
  "taxonomy": {
    "tier": "opt",
    "relations": [
      { "type": "depends", "target": "sovctl" }
    ]
  },
  "tags": ["service", "ai", "llm"],
  "upstream": {
    "name": "Ollama",
    "homepage": "https://ollama.com",
    "license": "MIT"
  },
  "contents": {
    "plugin": null,
    "gitops": {
      "kustomization": "k8s/kustomization.yaml",
      "resources": "k8s/"
    },
    "container": {
      "image": "ollama/ollama:latest",
      "compose": "docker/compose.yaml"
    }
  }
}
