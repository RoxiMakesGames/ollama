{
  "$schema": "https://raw.githubusercontent.com/RoxiMakesGames/sovpak/main/schema/sovpak-v3.schema.json",
  "id": "ollama",
  "name": "Ollama LLM",
  "version": "1.0.0",
  "description": "Local LLM inference via Ollama \u2014 models, chat, and API access",
  "license": "AGPL-3.0",
  "author": {
    "name": "Sovereign Infrastructure Team",
    "url": "https://opensovereign.dev"
  },
  "kind": "package",
  "taxonomy": {
    "tier": "opt",
    "relations": [
      {
        "type": "depends",
        "target": "sovctl"
      }
    ]
  },
  "tags": [
    "service",
    "ai",
    "llm"
  ],
  "upstream": {
    "name": "Ollama",
    "homepage": "https://ollama.com",
    "license": "MIT"
  },
  "contents": {
    "plugin": null,
    "gitops": {
      "kustomization": "k8s/kustomization.yaml",
      "resources": "k8s/"
    },
    "container": {
      "image": "ollama/ollama:latest",
      "compose": "docker/compose.yaml"
    }
  }
}
