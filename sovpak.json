{
  "$schema": "https://sovereign.dev/schemas/package/4.0.0",
  "id": "ollama",
  "name": "Ollama LLM",
  "version": "1.0.0",
  "description": "Local LLM inference via Ollama â€” models, chat, and API access",
  "license": "AGPL-3.0",
  "author": {
    "name": "Sovereign Infrastructure Team",
    "url": "https://opensovereign.dev"
  },
  "kind": "package",
  "taxonomy": {
    "tier": "opt",
    "relations": [
      {
        "type": "depends",
        "target": "sovctl"
      }
    ]
  },
  "tags": [
    "service",
    "ai",
    "llm"
  ],
  "upstream": {
    "name": "Ollama",
    "homepage": "https://ollama.com",
    "license": "MIT"
  },
  "deployments": [
    {
      "platform": "kubernetes",
      "path": "infra/",
      "files": {
        "kustomization": "kustomization.yaml"
      }
    },
    {
      "platform": "docker",
      "path": "docker/",
      "files": {
        "compose": "compose.yaml"
      }
    }
  ]
}
